{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sofar, we have seen how to make calculations with torch, and how to build a datagenerator. \n",
    "\n",
    "So, in theory we have enough knowledge to deliver the data in batches to our machine learning model to perform calculations on the data.\n",
    "\n",
    "But how to adjust the weights? How does the model learn which weights should be adjusted in which direction?\n",
    "\n",
    "\n",
    "Let's start with guessing the weights $w$ and $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([2.], requires_grad=True)\n",
    "b = torch.tensor([6.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `requires_grad`, by which we are telling torch to keep track of all calculations, \n",
    "so that we can calculate the gradient later on.\n",
    "\n",
    "We create a new output tensor $Q$ with a calculation:\n",
    "$$\n",
    "Q = w x + b\n",
    "$$\n",
    "\n",
    "This is our output, with the variables we have guessed.\n",
    "\n",
    "\n",
    "first we need some data $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8., 10., 12.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "Q = w * x + b\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a certain outcome. But how do we know if this is correct? For that, we need \n",
    "\n",
    "- some sort of ground truth.\n",
    "- a way to calculate the error\n",
    "\n",
    "A common way to calculate the error is the Mean Square Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y: torch.Tensor, yhat: torch.Tensor) -> torch.Tensor:\n",
    "    return ((y - yhat)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets assume the real values for w and b are 4 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.,  9., 13.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 4 * x + 1\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, you don't have access to these \"real\" values. You only have access to the outcome $y$, and your guess is that this\n",
    "outcome is produced something that is close to a model, in our case the linear model.\n",
    "\n",
    "We can compare the error with our estimates of $w$ and $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(y, Q)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have some loss, in this case, a total loss of 3.66. We want to minimize this loss, and need to adjust our guessed weights in order to do so.\n",
    "\n",
    "During training, we need the gradients of the error as defined by the loss function $\\mathcal{L}$ with respect to the parameters $w$ and $b$. This means we want:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$$\n",
    "\n",
    "With the `.backward()` method, torch will calculate all the derivatives. They are stored in the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could calculate the derivatives by hand, which is tedious, especially if you have many nested calculations. But because our two parameters `w` and `b` where marked with `requires_grad=True`, the gradient was tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.3333]), tensor([2.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see? Calling `.backward()` modified the loss in the parameters that are tracked with `requires_grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we would adjust the weights by a certain factor, the learning rate. Typically this is set to `1e-3` , but it can be as big as `1e-1` and as small as `1e-5`. \n",
    "\n",
    "It can even vary during training: you start with `1e-1`, and if the improvement of the learning slows down you decrease the learning rate with a certain factor, e.g. to `1e-2`.\n",
    "\n",
    "Lets adjust the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.8667], grad_fn=<SubBackward0>),\n",
       " tensor([5.8000], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "w = w - learning_rate * w.grad \n",
    "b = b - learning_rate * b.grad\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run a new prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3185, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = w * x + b\n",
    "loss = mse(y, Q)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked! Our loss is lower!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the adjustment of the weights, the training continues:\n",
    "\n",
    "- make a prediction\n",
    "- calculate the loss\n",
    "- calculate the gradients\n",
    "- adjust the weights with respect to the error with a certain rate\n",
    "\n",
    "And this is how the model learns!\n",
    "\n",
    "We would need an optimizer to properly reset the accumulated gradients if we want a true learning loop, which we will see later on.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deep-learning-wM7qE7ca-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9384df97cb25cd0ffeadd8ca5fc8c3b92d252d40e81804b4c63c6d046c91939e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
