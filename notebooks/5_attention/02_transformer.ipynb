{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "A final ingredient for the transformer architecture is positional encoding.\n",
    "\n",
    "With Attention we loose all sense of location in a sentence. A way around this is to add positional encoding.\n",
    "\n",
    "The idea is to add multiple sine waves.\n",
    "One sinewave will have a very fast frequency, another sinewave will have a slow frequency.\n",
    "Note that this is exactly how our time-system works: minutes have a fast frequency, hours a slower one, days of the week even slower etc.\n",
    "\n",
    "By combining a lot of sinewaves the model is able to figure out something about the location in time, relative to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_seq_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_seq_len, d_model)\n",
    "        # batch, seq_len, d_model\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "encod_block = PositionalEncoding(d_model=48, dropout=0.1, max_seq_len=96)\n",
    "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(pe, cmap=\"RdGy\", extent=(1, pe.shape[1] + 1, pe.shape[0] + 1, 1))\n",
    "# fig.colorbar(pos, ax=ax)\n",
    "ax.set_xlabel(\"Position in sequence\")\n",
    "ax.set_ylabel(\"Hidden dimension\")\n",
    "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
    "ax.set_xticks([1] + [i * 10 for i in range(1, 1 + pe.shape[1] // 10)])\n",
    "ax.set_yticks([1] + [i * 10 for i in range(1, 1 + pe.shape[0] // 10)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now completely recreate the transformer architecture. \n",
    "\n",
    "We will build the encoder-block (inside the dotted line, on the left). The encoder-decoder architecture is typically used for translation tasks. But we can use the encoder block for other tasks too, like classification or regression.\n",
    "\n",
    "\n",
    "<img src=transformers.pbm width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        x = self.layer_norm1(x + identity)\n",
    "        identity = x.clone()\n",
    "        x = self.ff(x)\n",
    "        x = self.layer_norm2(x + identity)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(config[\"hidden\"], config[\"dropout\"])\n",
    "\n",
    "        # Create multiple transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(config[\"hidden\"], config[\"num_heads\"], config[\"dropout\"])\n",
    "            for _ in range(config[\"num_blocks\"])\n",
    "        ])\n",
    "\n",
    "        self.out = nn.Linear(config[\"hidden\"], config[\"output\"])\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # Apply multiple transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random torch tensor with batch size 32, sequence length 64 and 128 features\n",
    "# note that the hidden dimensions of the transformer should be equal to the feature dimensions\n",
    "X = torch.randn(32, 64, 128)\n",
    "\n",
    "config = {\n",
    "    \"hidden\": 128,\n",
    "    \"dropout\": 0.1,\n",
    "    \"num_heads\": 4,  # sequence length should be divisible by num_heads\n",
    "    \"num_blocks\": 3,\n",
    "    \"output\": 2,\n",
    "}\n",
    "\n",
    "transformer = Transformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer(X)\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mads-deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
