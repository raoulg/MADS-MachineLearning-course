{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers as tk\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Lowercase, StripAccents, Sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We start with a simple corpus of two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The cat sat on the mat\", \"Where is the cat?\", \"The cat is blasé\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "normalizer.normalize_str(corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NFD (Normalization Form D) normalizer tokenizer performs Unicode normalization on strings, specifically converting them into the Canonical Decomposition form.\n",
    "Tokenizers use NFD normalization to ensure consistency in token representation, which is crucial for training effective machine learning models:\n",
    "\n",
    "- Standardization: It guarantees that different ways of decomposing the same character \n",
    "- Vocabulary Reduction: By decomposing characters, it can simplify the model's vocabulary. Instead of having separate tokens for 'a', 'aˊ', 'aˋ', etc., the tokenizer might be able to create one token for the base 'a' and separate tokens for the combining accents, leading to a smaller, more efficient vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[UNK]\"])\n",
    "tokenizer.pre_tokenizer = Whitespace() # This pre-tokenizer simply splits using the following regex: \\w+|[^\\w\\s]+\n",
    "normalizer = Sequence([NFD(), StripAccents(), Lowercase()])\n",
    "# Seqeunce allows concatenating multiple other Normalizer as a Sequence. All the normalizers run in sequence in the given order.\n",
    "tokenizer.normalizer = normalizer\n",
    "tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our models wont handle strings. So, what we typically do is to `tokenize` the words. We will assign arbitrary integers to the words.\n",
    "\n",
    "First, we need to get all the words. So we split the strings on whitespace, which gives us the words in every sentence.\n",
    "\n",
    "Then we assign an integer to every word. We can do this by creating a dictionary that maps words to integers. We can then use this dictionary to convert the words to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a vocabulary, which is just a mapping from tokens to arbitrary integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer.encode(\"cat\")\n",
    "enc.ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer.encode(\"The cat is drinking\")\n",
    "enc.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default index is returned when we have unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can translate back to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(enc.ids, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are now able to map the sentence from strings to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = tokenizer.encode(corpus[0])\n",
    "tokenized_sentence.ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you \"read\" the original sentence? You can use the vocab to translate back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ok, now, how to represent this. A naive way would be to use a one hot encoding.\n",
    "\n",
    "<img src=https://www.tensorflow.org/text/guide/images/one-hot.png width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "tokenized_tensor = torch.tensor(tokenized_sentence.ids)\n",
    "oh = F.one_hot(tokenized_tensor)\n",
    "oh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this might seem like a nice workaround, it is very memory inefficient. \n",
    "Vocabularies can easily grow into the 10.000+ words!\n",
    "\n",
    "So, let's make a more dense space. We simply decide on a dimensionality, and start with assigning a random vector to every word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://www.tensorflow.org/text/guide/images/embedding2.png width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"the vocabulary size is {vocab_size}\")\n",
    "hidden_dim = 4\n",
    "\n",
    "embedding = torch.nn.Embedding(\n",
    "    num_embeddings=vocab_size, embedding_dim=hidden_dim, padding_idx=-2\n",
    ")\n",
    "x = embedding(tokenized_tensor)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:\n",
    "\n",
    "- we started with a sentence of strings.\n",
    "- we map the strings to arbitrary integers\n",
    "- the integers are used with an Embedding layer; this is nothing more than a lookup table where every word get's a random vector assigned\n",
    "\n",
    "We started with a 6-word sentence. But we ended with a (6, 4) matrix of numbers.\n",
    "\n",
    "So, let's say we have a batch of 32 sentences. We can now store this for example as a (32, 15, 6) matrix: batchsize 32, length of every sentence is 15 (use padding if the sentence is smaller), and every word in the sentence represented with 6 numbers.\n",
    "\n",
    "This is exactly the same as what we did before with timeseries! We have 3 dimensional tensors, (batch x sequence_length x dimensionality) that we can feed into an RNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = x[None, ...]\n",
    "rnn = torch.nn.GRU(input_size=hidden_dim, hidden_size=16, num_layers=1)\n",
    "\n",
    "out, hidden = rnn(x_)\n",
    "out.shape, hidden.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem with Simple Tokenization\n",
    "Consider these two approaches:\n",
    "\n",
    "- Word-level tokenization: \"playing\" and \"played\" are treated as completely different tokens\n",
    "- Character-level tokenization: \"p\", \"l\", \"a\", \"y\", \"i\", \"n\", \"g\" are all separate tokens\n",
    "\n",
    "Both approaches have issues:\n",
    "\n",
    "- Word-level creates an enormous vocabulary and misses relationships between similar words\n",
    "- Character-level creates very long sequences and loses meaning\n",
    "\n",
    "## Enter BPE\n",
    "BPE is a clever middle ground that automatically learns to break words into meaningful subwords. Here's how it works:\n",
    "\n",
    "- Start with characters as your base vocabulary\n",
    "- Count all pairs of adjacent tokens in your training data\n",
    "- Merge the most frequent pair to create a new token\n",
    "- Repeat steps 2-3 until you reach your desired vocabulary size\n",
    "\n",
    "Initial text: `\"low lower lowest\"`. Initial tokens:\n",
    " - `[\"l\", \"o\", \"w\", \" \", \"l\", \"o\", \"w\", \"e\", \"r\", \" \", \"l\", \"o\", \"w\", \"e\", \"s\", \"t\"]`\n",
    "\n",
    "After first merge (most common pair \"l\" \"o\" → \"lo\"):\n",
    "- `[\"lo\", \"w\", \" \", \"lo\", \"w\", \"e\", \"r\", \" \", \"lo\", \"w\", \"e\", \"s\", \"t\"]`\n",
    "\n",
    "After second merge (\"lo\" \"w\" → \"low\"):\n",
    "- `[\"low\", \" \", \"low\", \"e\", \"r\", \" \", \"low\", \"e\", \"s\", \"t\"]`\n",
    "\n",
    "Lets see this in action on our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "normalizer = Sequence([NFD(), StripAccents(), Lowercase()])\n",
    "# Seqeunce allows concatenating multiple other Normalizer as a Sequence. All the normalizers run in sequence in the given order.\n",
    "tokenizer.normalizer = normalizer\n",
    "tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "print((f\"the vocabulary size is {tokenizer.get_vocab_size()}\"))\n",
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer.encode(\"The cat is drinking\")\n",
    "enc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(enc.ids, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildBPE(corpus: list[str], vocab_size: int) -> tk.Tokenizer:\n",
    "    tokenizer = tk.Tokenizer(tk.models.BPE())\n",
    "    trainer = tk.trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=1,\n",
    "        special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\"],\n",
    "    )\n",
    "\n",
    "    # handle spaces better by removing the prefix space\n",
    "    tokenizer.pre_tokenizer = tk.pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    tokenizer.decoder = tk.decoders.ByteLevel()\n",
    "\n",
    "    # train the BPE model\n",
    "    tokenizer.train_from_iterator(corpus, trainer)\n",
    "    tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
    "    return tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mads-deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
