{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "srcdir = Path(\"../..\").resolve()\n",
    "print(f\"Adding {srcdir} to sys.path, this is necessary to import from src\")\n",
    "sys.path.insert(0, str(srcdir))\n",
    "print(sys.path)\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classsification Metrics\n",
    "## 1 Confusion Matrix\n",
    "To showcase a confusion matrix, let's load the Fashion MNIST data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we make a post-processing function that makes the label a 1 if the number is a four, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "class DummyPreprocessor:\n",
    "    def __call__(self, batch: List[Tuple]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batchsize = len(batch)\n",
    "        # we allocate some memory. This speeds up filling the batch\n",
    "        X = torch.zeros(size=(batchsize, 1, 28, 28)) \n",
    "        Y = []\n",
    "        i = 0\n",
    "        for x, y in batch:\n",
    "            X[i] = x\n",
    "            # the label will be True if 4, False otherwise\n",
    "            Y.append(int(y == 4))\n",
    "            i += 1\n",
    "        return X, torch.tensor(Y, dtype=torch.long)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can create a Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION, preprocessor=DummyPreprocessor)\n",
    "datasets = fashionfactory.create_dataset()\n",
    "train = datasets[\"train\"]\n",
    "valid = datasets[\"valid\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our $y$ consists of zeros and ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train[0]\n",
    "X.shape, y.shape, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our basic model.\n",
    "\n",
    "The number of outputs should be 2, because we have two classes.\n",
    "Let's run this with only 100 training-steps; that way we can see the progress better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = Path(\"modellogs/dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.settings import TrainerSettings, ReportTypes\n",
    "from src.models import metrics\n",
    "accuracy = metrics.Accuracy()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=3,\n",
    "    metrics=[accuracy],\n",
    "    logdir=logdir,\n",
    "    train_steps=100,\n",
    "    valid_steps=50,\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.GIN,],\n",
    "    scheduler_kwargs={\"factor\": 0.5, \"patience\": 5},\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamers = fashionfactory.create_datastreamer(batchsize=32)\n",
    "trainstreamer = streamers[\"train\"]\n",
    "validstreamer = streamers[\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import imagemodels, train_model\n",
    "\n",
    "model = imagemodels.CNN(2, (3,3), 32, 64)\n",
    "\n",
    "trainer = train_model.Trainer(\n",
    "    model=model,\n",
    "    settings=settings,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    traindataloader=trainstreamer.stream(),\n",
    "    validdataloader=validstreamer.stream(),\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    ")\n",
    "trainer.loop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naively, this looks very good. The first epoch is alreay at 90%. But not so fast... \n",
    "What would you expect from blind guessing? Exactly 90% accuracy if you would guess all 0! Because 90% is a 0!\n",
    "That gives us a different perspective on the 90 or 94% you might get... So we need something more complex than just accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "validdata = validstreamer.stream()\n",
    "for _ in tqdm(range(len(validstreamer))):\n",
    "    X, y = next(validdata)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1)  # we get the one with the highest probability\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect all predictions in a list, and flatten the list with a list comprehension.\n",
    "\n",
    "With this, we can use the confusion matrix from sklearn and plot it with seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_curve, roc_curve \n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "plot = sns.heatmap(cfm_norm, annot=cfm_norm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the target 0 is predicted as a 0 for 96% of the time. So that is going well.\n",
    "However, the target 1 (number is a four) is predicted as actually 1 is much worse! \n",
    "\n",
    "## 2 Precision, Recall and F1\n",
    "The F1-score is a useful metric for problems with unblanaced datasets; it returns a number between 0 and 1 that gives an indication of how well a model is doing in the classification task. Other useful metrics are *precision* and *recall*.\n",
    "\n",
    "**Precision**: how many of the samples *predicted* as positive are actually positive\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**Recall**: how many of *actual* positive samples are indeed predicted as positive\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**F-score**: the harmonic mean of precision and recall\n",
    "\n",
    "$$ F = 2 * \\frac{precision * recall}{precision + recall} $$\n",
    "\n",
    "The F1-score of the dummy classifier is zero! \n",
    "Lets first visualize the f1-metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "prec = np.linspace(0, 1, k) # k precision values\n",
    "rec = np.linspace(0, 1, k) # k recall values\n",
    "\n",
    "def f1(prec, rec): # the f1 metric\n",
    "    return 2 * (prec*rec)/(prec+rec)\n",
    "\n",
    "from itertools import product\n",
    "combinations = [f1(p,r) for p in prec for r in rec] # a list of combinations\n",
    "grid = np.reshape(combinations, (k,k)) # reshaped into a grid\n",
    "sns.heatmap(grid) # and heatmapped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense: if one of precision or recall is low, the outcome is low. If both are high, the outcome is high. This is a balanced mixture of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, yhat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the f1 score is much more realistic than the accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to look at precision-recall:\n",
    "\n",
    "- recall describes how many correct items you actually retrieved, of all items you could have retrieved\n",
    "- precision describes of the items you labeled, you actually labeled them correct.\n",
    "\n",
    "For this, we are not going to look at the argmax, but directly at the logits of one of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "\n",
    "for _ in tqdm(range(len(validstreamer))):\n",
    "    X, y = next(validdata)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat[:, 1] # we get the probability of being a 4\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y, yhat)\n",
    "n = len(thresholds)\n",
    "sns.lineplot(x=thresholds, y=precision[:n], label=\"precision\")\n",
    "sns.lineplot(x=thresholds, y=recall[:n], label=\"recall\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how pushing the threshold up gives you a higher precision, but a lower recall, and the other way around. This is always a trade-off (do you understand why?)\n",
    "\n",
    "Another way to visualize this trade off is with a ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y, yhat)\n",
    "plot = sns.lineplot(x=fpr, y=tpr)\n",
    "plot.set(xlabel=\"FPR\", ylabel=\"TPR\")\n",
    "plt.plot([0,1], [0,1], \"k--\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the x-axis you see the False Positive Rate. In the beginning, the threshold is very low, so because you dont select any items your FPR is also low. But the same goes for your True Positive Rate.\n",
    "\n",
    "As you increase the threshold, you start to find True Positives. This is good, but it comes with a price: you will also select False Positives. You can see that, as the TPR starts to rise, the FPR also rises.\n",
    "\n",
    "Again, you are free to select a threshold to get any TPR you like. But the point of this curve is to maximize the area under the curve. In other words: the dotted diagonal line is what you would expect from blind guessing. Everything above the dotted line is an improvment beyond guessing. Models with a line that comes closer to the upper left corner are better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch to the multi-label case. We drop the post-processing function, and train with 10 classes again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=32)\n",
    "trainstreamer = streamers[\"train\"]\n",
    "validstreamer = streamers[\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = imagemodels.CNN(10, (3,3), 32, 64)\n",
    "settings.epochs=10\n",
    "\n",
    "trainer = train_model.Trainer(\n",
    "    model=model,\n",
    "    settings=settings,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    traindataloader=trainstreamer.stream(),\n",
    "    validdataloader=validstreamer.stream(),\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    ")\n",
    "trainer.loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "validdata = validstreamer.stream()\n",
    "for _ in range(len(validstreamer)):\n",
    "    X, y = next(validdata)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1) # we get the one with the highest probability\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "plot = sns.heatmap(cfm_norm, annot=cfm_norm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very efficient way to get an overview of the 100 different confusion-cases you could get.\n",
    "\n",
    "At a glance, you can see which targets are mislabeled the most often, and if they are, what they are confused with for what percentage."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15911aab0965639e9482f052beb89e7ca291bb3f153727c5758e3fe9ad1321e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep-learning-xB8KIJr7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
