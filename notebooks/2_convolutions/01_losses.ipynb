{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses in general\n",
    "We have been using crossentropy loss. But what is that?\n",
    "\n",
    "First, let's go back to what we want to do with a loss function: we want to give the model feedback on how good or bad it is working. With that information, it is able to adjust it's weights with the gradient.\n",
    "\n",
    "So, it is really important! Using the wrong carrot/stick will cause your model to behave in completely different ways...\n",
    "\n",
    "The most basic example is: we have a real value $y$, and a prediction $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([3.0, 2.0, 5.0])\n",
    "yhat = torch.tensor([2.5, 3.4, 4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_np = y.numpy()\n",
    "yhat_np = yhat.numpy()\n",
    "\n",
    "# Create a Pandas DataFrame containing the data\n",
    "data = pd.DataFrame({'True Values': y_np, 'Predicted Values': yhat_np, 'Data Points': range(1, len(y_np) + 1)})\n",
    "\n",
    "# Melt the DataFrame to have a 'variable' and 'value' columns\n",
    "data_melted = data.melt(id_vars='Data Points', var_name='variable', value_name='value')\n",
    "\n",
    "# Plot the data using Seaborn's pointplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=data_melted, x='Data Points', y='value', hue='variable')\n",
    "plt.title('Comparison of True Values and Predicted Values')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Values')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "As you might be able to infer, this is a regression problem. We will cover the classification later on, because regression is a bit simpler. You can see, the predictions are off by a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y - yhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most naive way to calculate the loss, is to look at the difference. Problem with this, is that if you take the mean, you run into problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y-yhat).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is very low, almost zero! A solution to this is to look at the absolute values, so that negative and positive values don't cancel out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y-yhat).abs().mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what is called: Mean Average Error. However, this does typically not work that well as a loss function. The default for regression is a variation on this, the Mean Squared Error:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$$\n",
    "\n",
    "This is the mean $\\frac{1}{n}\\sum_{i=1}^n$ of the squared error $(Y_i - \\hat{Y}_i)^2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y-yhat)**2).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But torch has already implemented that for us in an optimized way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "loss(yhat, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "For classification, we have to pull a bit more tricks, because models typically not give discrete output, but real valued output (floating point numbers).\n",
    "\n",
    "Let's say you have five classes.\n",
    "One way you could do this, is to predict the probability of every class.\n",
    "The trick that is commonly used, is to have five output nodes. Every nodes gives a number, and you tell the model that the number should be high in the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "n_observations = 3\n",
    "n_classes = 5\n",
    "input = torch.randn(n_observations, n_classes, requires_grad=True)\n",
    "model = torch.nn.LogSoftmax(dim=1)\n",
    "yhat = model(input)\n",
    "print(f\"the shape of the output is {yhat.shape}\")\n",
    "yhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three predictions, every prediction has 5 values. We applied a LogSoftmax layer to the output, which gives us the log of a softmax. A softmax function scales all the values, such that the sum is 1 (which is what you need for probabilities: it would be nonsense if you have a chance above 100% of something happening, right?)\n",
    "\n",
    "You can verify for yourself that this sums to 1 for every observation, by taking the exponent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress Seaborn warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n",
    "\n",
    "probabilities = torch.exp(yhat).detach().numpy()\n",
    "\n",
    "# Create a Pandas DataFrame containing the data\n",
    "data = pd.DataFrame(probabilities, columns=[f'Class {i}' for i in range(1, probabilities.shape[1] + 1)])\n",
    "data['Observation'] = range(1, probabilities.shape[0] + 1)\n",
    "\n",
    "# Melt the DataFrame to have a 'variable' and 'value' columns\n",
    "data_melted = data.melt(id_vars='Observation', var_name='Class', value_name='Probability')\n",
    "\n",
    "g = sns.FacetGrid(data_melted, col='Observation', col_wrap=3, height=4, aspect=1)\n",
    "\n",
    "# Create the bar plots\n",
    "g.map(sns.barplot, 'Class', 'Probability', order=[f'Class {i}' for i in range(1, probabilities.shape[1] + 1)], palette='tab10')\n",
    "\n",
    "# Add titles and labels\n",
    "g.fig.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle('Softmax Probabilities for Each Class and Observation', fontsize=16)\n",
    "g.set_axis_labels('Class', 'Probability')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say the real classes are these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([0, 1, 4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic loss function is the negative log likelihood. The function is:\n",
    "\n",
    "$$NLL = - log(\\hat{y}[c])$$\n",
    "\n",
    "Or, in plain language: take the probabilities $\\hat{y}$, and pick the probability of the correct class $c$ from the list of probabilities with $\\hat{y}[c]$. Now take the log of that.\n",
    "\n",
    "The log has the effect that predicting closer to 0 if it should have been 1 is punished extra.\n",
    "\n",
    "In our case, for the first observation, this means that we get $-log(0.26)$ (because the probability for the 0th class is 0.26). Because we used the LogSoftmaxed, we don't need to take the log and can just take $-(-1.3472)$ as the loss for our first case. For the second case, we have 0.6816 at the 1th index, of which the log is -0.3834. You can see, that the higher the probability, the closer to zero the loss value will be. We take the - value, because we want to minimize the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.NLLLoss()\n",
    "loss(yhat, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that this is equivalent if we do that manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.zeros(3)\n",
    "for observation, c in enumerate(y):\n",
    "    loss[observation] = -yhat[observation, c]\n",
    "loss.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, CrossEntropyLoss does the same, but it adds the LogSoftmax to the loss. This means you don't need to add a LogSoftmax layer to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "loss(input, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel\n",
    "\n",
    "Now, what about the cases where your model can predict more than one class at once?\n",
    "\n",
    "Let's say you have a model that looks at lung photos to determine a disease. You can predict three cases: lungcancer, pneumonia (longontsteking), pneumothorax (klaplong).\n",
    "\n",
    "Now, let's imagine you have an unlucky patient with more than one condition. How would you want the model to predict this?\n",
    "\n",
    "We will encode this with a multi hot encoding.\n",
    "The prediction should be high, if there is a 1 in the target $y$, and low if there is a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([0., 1., 1.])\n",
    "\n",
    "torch.manual_seed(7)\n",
    "input = torch.randn(3, requires_grad=True) * 2\n",
    "input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, both the chance at pneumonia and a pneumothorax are high values. But, a softmax wil ruin this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.softmax(dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sums to 1, but that is not what you want!\n",
    "What you need is a sigmoid: this will scale everything between 0 and 1, but without making everything sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Sigmoid()\n",
    "yhat = m(input)\n",
    "yhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The loss function that goes with this is Binary Cross Entropy. A simplified version looks like this\n",
    "\n",
    "$$ BCE = \\frac{1}{n}\\sum_{i=1}^n y_i \\cdot log(\\hat{y}_i) + (1-y_i) \\cdot log(1-\\hat{y}_i) $$\n",
    "\n",
    "Or, in plain language: \n",
    "- assume that $y$ is a binary label (0 or 1)\n",
    "- predict the probability $\\hat{y}$\n",
    "- if the label is 1, take the log of the probability: $y_i \\cdot log(\\hat{y}_i$)\n",
    "- if the label is 0, take the log of $1-\\hat{y}$\n",
    "- take the mean $\\frac{1}{n}\\sum_{i=1}^n$ of that\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.BCELoss()\n",
    "loss(yhat, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.zeros(3)\n",
    "for i in range(len(y)):\n",
    "    loss[i] = -(y[i] * torch.log(yhat[i]) + (1-y[i]) * torch.log(1-yhat[i]))\n",
    "loss.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case you dont have predictions with values between 0 and 1, you can use the WithLogits variation. You can then skip the sigmoid layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "loss(input, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapup\n",
    "\n",
    "Losses are very important: they tell your model what is \"right\" and \"wrong\" and determines what the model will learn!\n",
    "\n",
    "- For regression models, typically use a MSE\n",
    "- For classification, use CrossEntropyLoss (note: this might be implemented different in other libraries like Tensorflow!)\n",
    "- For multilabel, use BinaryCrossEntropy\n",
    "\n",
    "There are other, more complex losses for more complex usecases but these three will cover 80% of your needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
