{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.data import make_dataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = Path(\"../../data/raw/\")\n",
    "train_dataloader, test_dataloader = make_dataset.get_MNIST(datadir, batch_size=64) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain an item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_dataloader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image follows the channels-first convention: (channel, width, height). The label is an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets pull this through a Conv2d layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(\n",
    "    in_channels=1, \n",
    "    out_channels=32,\n",
    "    kernel_size=3,\n",
    "    padding=(1,1))\n",
    "out = conv(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening here? Can you explain all the parameters, and relate them to the outputshape?\n",
    "\n",
    "Let's see what happens if we change the padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 26, 26])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(\n",
    "    in_channels=1, \n",
    "    out_channels=32,\n",
    "    kernel_size=3,\n",
    "    padding=(0,0))\n",
    "out = conv(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we change the stride from the default 1 to 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 14, 14])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(\n",
    "    in_channels=1, \n",
    "    out_channels=32,\n",
    "    kernel_size=3,\n",
    "    padding=(1,1),\n",
    "    stride=2)\n",
    "out = conv(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you need to think about what is going in and out of the convolution. We can stitch multiple layers together like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolutions = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    ")\n",
    "out = convolutions(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dimensions of the featuremap have become really small. You need to take this into account: If we would have started with a smaller image, we could get errors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Calculated padded input size per channel: (2 x 2). Kernel size: (3 x 3). Kernel size can't be greater than actual input size\n"
     ]
    }
   ],
   "source": [
    "x_too_small = torch.rand((32, 1, 12, 12))\n",
    "\n",
    "try:\n",
    "    convolutions(x_too_small)\n",
    "except RuntimeError as err:\n",
    "    print(\"ERROR:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point our `out` has 32 activation maps, each 2x2 big.\n",
    "\n",
    "If we want to pull the activation maps through a neural network (A dense layer) we will need to flatten them (do you understand what happens if you dont do that?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_nn = nn.Flatten()(out)\n",
    "input_nn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "CNN(\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=32, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "model = CNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29482"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import train_model\n",
    "train_model.count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 30k parameters. You will always need to judge that relative to your input data: how many observations do you have? Do you think the model needs a lot of complexity, or not so much?\n",
    "\n",
    "What is the trade off between adding more complexity? Or reducing complexity?\n",
    "\n",
    "Try to answer this trade of in terms of:\n",
    "\n",
    "- speed\n",
    "- generalization\n",
    "- accuracy\n",
    "\n",
    "We will need to tell the model how good it is performing. To do that, we will need to pick a loss function $\\mathcal{L}$. We will discuss this in more depth, but for now, just take my word for it that a CrossEntropyLoss is a good pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 17:20:01.467 | INFO     | src.data.data_tools:clean_dir:164 - Clean out ../../models/test\n",
      "2022-05-05 17:20:33.831 | INFO     | src.models.train_model:trainloop:75 - Epoch 0 train 0.0114 | test 0.0082 acc 0.8092\n",
      "2022-05-05 17:21:06.732 | INFO     | src.models.train_model:trainloop:75 - Epoch 1 train 0.0070 | test 0.0066 acc 0.8428\n",
      "2022-05-05 17:21:41.725 | INFO     | src.models.train_model:trainloop:75 - Epoch 2 train 0.0059 | test 0.0060 acc 0.8642\n",
      "2022-05-05 17:22:11.627 | INFO     | src.models.train_model:trainloop:75 - Epoch 3 train 0.0054 | test 0.0058 acc 0.8672\n",
      "2022-05-05 17:22:41.427 | INFO     | src.models.train_model:trainloop:75 - Epoch 4 train 0.0050 | test 0.0058 acc 0.8637\n",
      "2022-05-05 17:23:10.736 | INFO     | src.models.train_model:trainloop:75 - Epoch 5 train 0.0047 | test 0.0054 acc 0.8766\n",
      "2022-05-05 17:23:39.956 | INFO     | src.models.train_model:trainloop:75 - Epoch 6 train 0.0045 | test 0.0047 acc 0.8933\n",
      "2022-05-05 17:24:09.413 | INFO     | src.models.train_model:trainloop:75 - Epoch 7 train 0.0043 | test 0.0050 acc 0.8867\n",
      "2022-05-05 17:24:41.715 | INFO     | src.models.train_model:trainloop:75 - Epoch 8 train 0.0041 | test 0.0047 acc 0.8902\n",
      "2022-05-05 17:25:14.768 | INFO     | src.models.train_model:trainloop:75 - Epoch 9 train 0.0040 | test 0.0043 acc 0.8936\n"
     ]
    }
   ],
   "source": [
    "model = train_model.trainloop(\n",
    "    epochs=10,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    log_dir=\"../../models/test/\",\n",
    "    eval_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26ecd70c250b5fc210d88fc9e1a14b5d0dd9aee31ae2ed2f5669e6e14392ba9e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('deep-learning-xB8KIJr7-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
