{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow for a data science project will follow these lines:\n",
    "\n",
    "1. Get and explore the data\n",
    "2. Build a model \n",
    "3. Train the model\n",
    "4. Save and predict\n",
    "\n",
    "## 1. Get and Explore the Data\n",
    "The first step can take quite some time; data quality is often something that needs to be checked, and correlations between data should often be explored and visualized.\n",
    "\n",
    "This step can be a full project on its own: you clean the data, make sure you can access it properly, and create visualizations and hypothesis to gain insight into the data that can be shown in a dashboard.\n",
    "\n",
    "The insight in the data is an essential ingredient for deciding on a model.\n",
    "\n",
    "## 2. Build a model\n",
    "Based on domain knowledge and a first exploration of the data, a model can be selected.\n",
    "\n",
    "Sometimes, the relation between features and outcome is very obvious. You might have features that\n",
    "correlate very high with the outcome variable, and a domain expert confirms that the correlations make sense.\n",
    "\n",
    "If this is the case, you can often build a simple model. If you expect to have non-linear and complex interactions between the features,\n",
    "you could use a model that works with non-linear data like a SVM plus kernel, or a random forest.\n",
    "\n",
    "If you have enough data (as a rule of thumb, a lower threshold of 1000 observations) you can consider a neural network architecture.\n",
    "If the expected complexity of the data is low, you can use a relative small network.\n",
    "If you have lots and lots of data with a high complexity, you should consider to increase the complexity of your model too.\n",
    "\n",
    "How you can build a model, and what suitable models are for different datatypes and situations, will be the subject of the whole course.\n",
    "\n",
    "## 3. Train the model\n",
    "Once you created a model, it hasnt learned anything yet. The model must be trained to learn the right connections, a bit like a baby that has to learn about what works and what doesn't.\n",
    "\n",
    "In this notebook, I will introduce you to PyTorch. Another high level library is Tensorflow, which is used a lot too.\n",
    "While the interface is comparable, the Tensorflow syntax is a bit more high-level. While this can be an advantage, \n",
    "it also has a downside: at the moment you ever need to dive a bit deeper into the architecture itself, it is much harder to\n",
    "add something new with TensorFlow, compared to PyTorch.\n",
    "\n",
    "## 4. Save and predict\n",
    "Finally, you will want to use the trained model to predict new observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "We will use the fashion MNIST dataset. You will find this dataset a lot in machine learning tutorials. It are small (28x28) images of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.models import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../../data/raw/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973e2bab99d742e58a3e8299306c9336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/raw/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../../data/raw/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../../data/raw/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590869bb76f64b03bb66ffe4054eccd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/raw/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../../data/raw/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../../data/raw/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265a425d269a40cbae97983a63d5b4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/raw/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/raw/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../../data/raw/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40161ade83be456bb4fd43c6c6ad00c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/raw/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/raw/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "\n",
    "from pathlib import Path\n",
    "datadir = \"../../data/raw/\"\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have `torch.datasets`. They implement at minimum an `.__getitem__` and `.__len__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data, we can use the __getitem__ method by calling an index, just like you would do with a list or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, torch.Tensor, int)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = training_data[0]\n",
    "type(x), type(x[0]), type(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = training_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is a tuple. We can check the length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the 0th item, which is the image (tensor). The other item is the label (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = x[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the image has a channel-first convention: it is a 28x28 pixel image, and it has 1 channel (grey). Look into the official documentation if you want to know more about datasets and how to build your own: [docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Ok, we want to batch this into a dataloader. From the documentation:\n",
    "\n",
    "> The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the length of the dataloader different from the dataset? We had 60000 items before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 157)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do we see here? Our datashape has four dimensions:\n",
    "\n",
    "- 64: this is the batch size. Every batch has 64 observations; in this case 64 images\n",
    "- 1: this is the channel. Colorimages typically have 3 channels. Our images have just one color, and thus 1 channel. So images can have more channels (e.g. infrared etc)\n",
    "- (28,28) : this is the actual image, with dimensions 28x28\n",
    "\n",
    "Lets visualize the first example, the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = X[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2bfeb4550>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you recognize the setup from the `linearmodel` notebook. \n",
    "\n",
    "- We will `Flatten` the image. That means we will transform our (64, 1, 28, 28) data into (64, 784) shaped data. What we do here, is flattening the image into a one dimensional vector.\n",
    "- We have a stack of hidden layers. These are essentially dotproducts. Our vector of 784 (28*28) elements is transformed into 512 elements, and then into 10 elements because we have 10 classes.\n",
    "- in between the linear transformations you can see the activation functions,here a `ReLu` \n",
    "- The `forward` method is what is called during training. This gives you control over the flow of information: it is easy to create some parallel flow of data if you want to do something like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an optimizer. We will dive into this in later lessons.\n",
    "\n",
    "For now, it is enough to know this:\n",
    "\n",
    "Your model makes a prediction. But how does the model know if it is right, or wrong?\n",
    "And, more specific: how does the model know which weights it needs to modify in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 60000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"../../models/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 16:04:01.282 | INFO     | src.data.data_tools:dir_add_timestamp:228 - Logging to ../../models/test/20221117-1604\n",
      "100%|██████████| 50/50 [00:00<00:00, 170.47it/s]\n",
      "2022-11-17 16:04:01.810 | INFO     | src.models.train_model:trainloop:171 - Epoch 0 train 1.0523 test 0.6830 metric ['0.7381']\n",
      "100%|██████████| 50/50 [00:00<00:00, 175.53it/s]\n",
      "2022-11-17 16:04:02.233 | INFO     | src.models.train_model:trainloop:171 - Epoch 1 train 0.6387 test 0.5851 metric ['0.7881']\n",
      "100%|██████████| 50/50 [00:00<00:00, 182.16it/s]\n",
      "2022-11-17 16:04:02.647 | INFO     | src.models.train_model:trainloop:171 - Epoch 2 train 0.5669 test 0.5106 metric ['0.8153']\n",
      "100%|██████████| 50/50 [00:00<00:00, 188.00it/s]\n",
      "2022-11-17 16:04:03.051 | INFO     | src.models.train_model:trainloop:171 - Epoch 3 train 0.5504 test 0.5076 metric ['0.8144']\n",
      "100%|██████████| 50/50 [00:00<00:00, 172.25it/s]\n",
      "2022-11-17 16:04:03.481 | INFO     | src.models.train_model:trainloop:171 - Epoch 4 train 0.4725 test 0.5008 metric ['0.8231']\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.37it/s]\n"
     ]
    }
   ],
   "source": [
    "model, test_loss = train_model.trainloop(\n",
    "    epochs=5,\n",
    "    model=model,\n",
    "    optimizer=optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=[accuracy],\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=50,\n",
    "    eval_steps=50,\n",
    "    tunewriter=[\"tensorboard\", \"gin\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = Path(\"../../models\") \n",
    "model_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = model_dir / \"trained_model\"\n",
    "torch.save(model, modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a batch $X$, $y$ and make a prediction $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(test_dataloader))\n",
    "yhat = loaded_model(X)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy:\n",
    "- for every example we have 10 numbers\n",
    "- the location with the highest value is the prediction\n",
    "- we can get the index with `argmax` over dimension 1\n",
    "- we compare that index with the original number\n",
    "- This gives us a count of all the correct predictions\n",
    "- dividing that through the total length gives us the accuracy percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (yhat.argmax(dim=1) == y).sum() / len(y)\n",
    "acc.item() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the accuracy for a single batch! \n",
    "Get another batch by running next() in the cell above, and calculate the accuracy again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 16:04:03.626 | INFO     | src.data.data_tools:clean_dir:195 - Clean out ../../models/test\n"
     ]
    }
   ],
   "source": [
    "cleanup = True\n",
    "from src.data import data_tools\n",
    "# to remove the trained model\n",
    "if cleanup:\n",
    "    modelpath.unlink()\n",
    "    data_tools.clean_dir(log_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45c41bdaf5373703b03bba2d9bd89c97dc8ee5add9f1112e039ff04603b8e2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
