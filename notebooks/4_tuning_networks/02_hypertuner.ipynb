{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'plotly_mimetype+notebook'\n",
    "\n",
    "import visualize\n",
    "DELETE = True # to delete the tunedir at the end of the notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a general reference notebook to explore the use of ray tuner\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very simple function to hypertune"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a basic function:\n",
    "$$ f(x) = ax^3 + bx^2 + cx $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def model(x, config):\n",
    "    \"\"\"\n",
    "    The model is a function that takes in some input x and a configuration.\n",
    "    The configuration has parameters changes the output.\n",
    "    To keep things simple, this model does not has learnable parameters like\n",
    "    our models would usually have\n",
    "    \"\"\"\n",
    "    return (\n",
    "        config[\"a\"] * x**3\n",
    "        + config[\"b\"] * x**2\n",
    "        + config[\"c\"] * x\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to know: what are optimal values for $a$, $b$ and $c$ such that the\n",
    "mean is minimized, or maximized? Let's test\n",
    "some values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 40 # we run 40 experiments\n",
    "NUM_DATA = 200 # our data has 200 observations\n",
    "MAX_ITER = 15 # we run every experiment for a max of 15 epochs\n",
    "MODE = \"max\" # we want to maximize the mean. This can also be \"min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, NUM_DATA)\n",
    "y1 = model(x, dict(a=-2, b=2, c=2))\n",
    "y2 = model(x, dict(a=1.2, b=-3.5, c=2))\n",
    "plt.plot(x, y1, label=\"y1\")\n",
    "plt.plot(x, y2, label=\"y2\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.mean(), y2.mean()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to hypertune this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate a 100 datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random datapoints, uniform distributed on the domain [-2, 2]\n",
    "np.random.seed(42)\n",
    "data = np.random.uniform(-2, 2, NUM_DATA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train function and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import ray\n",
    "\n",
    "\n",
    "def train(config: Dict):\n",
    "    total = 0\n",
    "    np.random.seed(42)\n",
    "    # The seed is just for didactical purposes, to make the conclusions reproducable.\n",
    "    # In a real setting, you shouldnt use a seed while exploring hyperparameters!\n",
    "    # If you really want something deterministic, you need to change the\n",
    "    # seed every time you run, and store the seed in the settings.\n",
    "    np.random.shuffle(data)\n",
    "    for epoch in range(100):\n",
    "        loss = 0.0\n",
    "        # we run the model on the data\n",
    "        for i, x in enumerate(data):\n",
    "            score = model(x, config)\n",
    "            # calculate the loss\n",
    "            loss += score.mean()\n",
    "\n",
    "        # and log the loss to ray.tune\n",
    "        ray.train.report({\"mean_score\": loss / (len(data))})\n",
    "\n",
    "\n",
    "config = {\"a\": tune.uniform(-2, 2), \"b\": tune.uniform(-2, 2), \"c\": tune.uniform(-2, 2)}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a total score, and loop through the data.\n",
    "For every observation $x$ we test the function, and keep track of the score.\n",
    "The score is reported to `tune` with `tune.report`. We keep track of the\n",
    "iterations and of the mean score.\n",
    "\n",
    "Our config defines a uniform distribution for values of a, b and c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = {}\n",
    "best_config = {}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do a random search. The reporter shows some output to keep track of.\n",
    "The `tune.run` function runs the hypertuning. Our metric is the value of\n",
    "`mean_score`, which is what we report in `tune.report`. We want to maximize this\n",
    "value, so we tell tune to set `mode` to `\"max\"`.\n",
    "\n",
    "We will take 40 samples, and stop training after 100 iterations for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tune_dir = Path(\"../../models/ray/\")\n",
    "tune_dir.exists(), tune_dir.resolve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporter = JupyterNotebookReporter(overwrite=True)\n",
    "\n",
    "tic = time.time()\n",
    "analysis = tune.run(\n",
    "    train,\n",
    "    config=config,\n",
    "    metric=\"mean_score\",\n",
    "    mode=MODE,\n",
    "    local_dir=str(tune_dir.resolve()),\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    stop={\"training_iteration\": MAX_ITER},\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "timer[\"ray_random\"] = time.time() - tic\n",
    "best = analysis.get_best_config()\n",
    "best[\"mean_score\"] = analysis.best_result[\"mean_score\"]\n",
    "best_config[\"random\"] = best\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we searched the hyperparameter space. Problem is, these spaces potentially can get\n",
    "pretty big. Let's imagine you have 10 hyperparameters, and every hyperparameter has 5\n",
    "possible (relevant) values, you already have $5^{10}$ possible combinations, which is almost 10 million. Even if checking of every configuration would take just 1 second, it would take more than a 100 days to check them all...This\n",
    "space can grow out of control pretty fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resulting config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets run the objective with the best config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = analysis.get_best_config()\n",
    "y = model(data, config)\n",
    "plt.scatter(data, y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config is sampled at random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we visualize the sampled hyperparameter space, we can clearly see it is\n",
    "samples at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analysis.results_df\n",
    "plot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "plot = analysis.results_df\n",
    "select = [\"config/a\", \"config/b\", \"config/c\", \"mean_score\"]\n",
    "p = plot[select].reset_index()\n",
    "px.parallel_coordinates(p, color=\"mean_score\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the mean scores are sort of randomly distributed. This is a direct\n",
    "effect of random guessing parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we improve the search algorithm with a bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "\n",
    "bayesopt = BayesOptSearch(random_search_steps=NUM_SAMPLES)\n",
    "\n",
    "config = {\"a\": tune.uniform(-2, 2), \"b\": tune.uniform(-2, 2), \"c\": tune.uniform(-2, 2)}\n",
    "\n",
    "tic = time.time()\n",
    "analysis = tune.run(\n",
    "    train,\n",
    "    config=config,\n",
    "    metric=\"mean_score\",\n",
    "    mode=MODE,\n",
    "    local_dir=str(tune_dir.resolve()),\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    stop={\"training_iteration\": MAX_ITER},\n",
    "    search_alg=bayesopt,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "timer[\"ray_bayes\"] = time.time() - tic\n",
    "\n",
    "best = analysis.get_best_config()\n",
    "best[\"mean_score\"] = analysis.best_result[\"mean_score\"]\n",
    "best_config[\"bayes\"] = best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_timers(timer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analysis.results_df\n",
    "select = [\"config/a\", \"config/b\", \"config/c\", \"mean_score\"]\n",
    "p = plot[select].reset_index()\n",
    "px.parallel_coordinates(p, color=\"mean_score\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only is this slightly faster, you can also see that some scores are a bit more clustered.\n",
    ". In addition to that, the result is more often a bit\n",
    "better than random guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame.from_dict(best_config, orient=\"index\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperband\n",
    "\n",
    "Hyperband aborts runs early. Configs that are unpromising are abandoned before they complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "\n",
    "scheduler = AsyncHyperBandScheduler(\n",
    "    time_attr=\"training_iteration\", grace_period=1, reduction_factor=3, max_t=MAX_ITER\n",
    ")\n",
    "\n",
    "config = {\"a\": tune.uniform(-2, 2), \"b\": tune.uniform(-2, 2), \"c\": tune.uniform(-2, 2)}\n",
    "\n",
    "tic = time.time()\n",
    "analysis = tune.run(\n",
    "    train,\n",
    "    config=config,\n",
    "    metric=\"mean_score\",\n",
    "    mode=MODE,\n",
    "    local_dir=str(tune_dir.resolve()),\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    stop={\"training_iteration\": MAX_ITER},\n",
    "    scheduler=scheduler,\n",
    "    verbose=2,\n",
    ")\n",
    "timer[\"ray_hyperband\"] = time.time() - tic\n",
    "\n",
    "best = analysis.get_best_config()\n",
    "best[\"mean_score\"] = analysis.best_result[\"mean_score\"]\n",
    "best_config[\"hyperband\"] = best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_timers(timer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analysis.results_df\n",
    "select = [\"config/a\", \"config/b\", \"config/c\", \"mean_score\"]\n",
    "p = plot[select].reset_index()\n",
    "px.parallel_coordinates(p, color=\"mean_score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(plot[\"mean_score\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is better, too. You can see that there are more scores towards the maximum.\n",
    "You can also see that only some (the best) have been run for the maximum amount of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data=p, x=p.index, y=\"mean_score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(best_config, orient=\"index\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperbayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "\n",
    "bohb_hyperband = HyperBandForBOHB(\n",
    "    time_attr=\"training_iteration\",\n",
    "    max_t=MAX_ITER,\n",
    "    reduction_factor=3,\n",
    "    stop_last_trials=False,\n",
    ")\n",
    "\n",
    "\n",
    "config = {\"a\": tune.uniform(-2, 2), \"b\": tune.uniform(-2, 2), \"c\": tune.uniform(-2, 2)}\n",
    "\n",
    "bohb_search = TuneBOHB()\n",
    "\n",
    "tic = time.time()\n",
    "analysis = tune.run(\n",
    "    train,\n",
    "    config=config,\n",
    "    metric=\"mean_score\",\n",
    "    mode=MODE,\n",
    "    local_dir=str(tune_dir.resolve()),\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    stop={\"training_iteration\": MAX_ITER},\n",
    "    search_alg=bohb_search,\n",
    "    scheduler=bohb_hyperband,\n",
    "    verbose=2,\n",
    ")\n",
    "timer[\"ray_hyperbayes\"] = time.time() - tic\n",
    "\n",
    "best = analysis.get_best_config()\n",
    "best[\"mean_score\"] = analysis.best_result[\"mean_score\"]\n",
    "best_config[\"hyperbayes\"] = best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_timers(timer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analysis.results_df\n",
    "select = [\"config/a\", \"config/b\", \"config/c\", \"mean_score\"]\n",
    "p = plot[select].reset_index()\n",
    "px.parallel_coordinates(p, color=\"mean_score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(best_config, orient=\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = analysis.results_df[[\"training_iteration\", \"mean_score\"]]\n",
    "plt.scatter(data=p, x=\"training_iteration\", y=\"mean_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DELETE:\n",
    "    shutil.rmtree(tune_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-ho7aY0_Y-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "826f7c35c7cb2374ed015b71f995b28d51afc038e74920eb490e51986fe41e8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
