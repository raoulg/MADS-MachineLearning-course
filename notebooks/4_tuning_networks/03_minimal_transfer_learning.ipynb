{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<td>\n",
    "<a href=\"https://colab.research.google.com/github/raoulg/MADS-MachineLearning-course/blob/master/notebooks/4_tuning_networks/03_minimal_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is based on the PyTorch tutorial for transfer learning. The original tutorial can be found [here](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html).\n",
    "\n",
    "Transfer learning utilizes the idea that a model trained on a large dataset can be used as a starting point for a model trained on a smaller dataset. This is especially useful when the smaller dataset is not large enough to train a model from scratch. In this tutorial, we will use a pre-trained model to classify ants and bees. The dataset contains 120 training images and 75 validation images for each class.\n",
    "\n",
    "The two major transfer learning scenarios look as follows:\n",
    "\n",
    "-   **Finetuning the ConvNet**: Instead of random initialization, we\n",
    "    initialize the network with a pretrained network, like the one that\n",
    "    is trained on imagenet 1000 dataset. Rest of the training looks as\n",
    "    usual.\n",
    "-   **ConvNet as fixed feature extractor**: Here, we will freeze the\n",
    "    weights for all of the network except that of the final fully\n",
    "    connected layer. This last fully connected layer is replaced with a\n",
    "    new one with random weights and only this layer is trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if the notebook is running on Google Colab\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Running on Google Colab; install loguru\n",
    "    !pip install loguru\n",
    "else:\n",
    "    # Not running on Google Colab; you might be on a local setup\n",
    "    print(\"Not running on Google Colab. Make sure you have hardware accelaration available.\")\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import zipfile\n",
    "from loguru import logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hymenoptera():\n",
    "    datadir = Path.home() / \".cache/mads_datasets/hymenoptera_data\"\n",
    "    url = \"https://download.pytorch.org/tutorial/hymenoptera_data.zip\"\n",
    "    if not datadir.exists():\n",
    "        logger.info(f\"Creating directory {datadir}\")\n",
    "        datadir.mkdir(parents=True)\n",
    "\n",
    "        response = requests.get(url)\n",
    "        zip_file_path = datadir / \"hymenoptera_data.zip\"\n",
    "        with open(zip_file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        logger.info(f\"Extracting {zip_file_path}\")\n",
    "        with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(datadir)\n",
    "        zip_file_path.unlink()\n",
    "    else:\n",
    "        logger.info(f\"Directory {datadir} already exists, skipping download.\")\n",
    "    return datadir / \"hymenoptera_data\"\n",
    "\n",
    "datadir = get_hymenoptera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "=========\n",
    "\n",
    "We will use torchvision and torch.utils.data packages for loading the\n",
    "data.\n",
    "\n",
    "The problem we\\'re going to solve is training a model to classify\n",
    "**ants** and **bees**. We have about 120 training images each for ants\n",
    "and bees.\n",
    "\n",
    "There are 75 validation images for each class. Usually, this\n",
    "is too small of a dataset to generalize upon, if trained from scratch.\n",
    "Since we are using transfer learning, we should be able to generalize\n",
    "reasonably well.\n",
    "\n",
    "This dataset is a very small subset of imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders():\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    image_datasets = {x: datasets.ImageFolder(datadir / x, data_transforms[x]) for x in ['train', 'val']}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                                shuffle=True, num_workers=4)\n",
    "                for x in ['train', 'val']}\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    return dataloaders, dataset_sizes, class_names\n",
    "\n",
    "dataloaders, dataset_sizes, class_names = create_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    logger.warning(\"This model will take 15-20 minutes on CPU. Consider using accelaration, eg with google colab (see button on top of the page)\")\n",
    "logger.info(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a few images\n",
    "======================\n",
    "\n",
    "Let\\'s visualize a few training images so as to understand the data\n",
    "augmentations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, title):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "img = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(img, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we have been using the mltrainer module.\n",
    "However, to both make it easer to run this on google colab and to show you the full code of a training loop, I will define the functions for training and evaluating the model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, lossfn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss: float = 0.0\n",
    "    train_acc: float = 0.0\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(x)\n",
    "        loss = lossfn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, acc = torch.max(yhat, 1)\n",
    "        train_acc += torch.sum(acc == y.data)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, lossfn, optimizer, scheduler, device):\n",
    "    model.eval()\n",
    "    test_loss: float = 0.0\n",
    "    test_acc: float = 0.0\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        yhat = model(x)\n",
    "        loss = lossfn(yhat, y)\n",
    "        test_loss += loss.item()\n",
    "        _, acc = torch.max(yhat, 1)\n",
    "        test_acc += torch.sum(acc == y.data)\n",
    "    scheduler.step(test_loss)\n",
    "    return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, lossfn, optimizer, scheduler, num_epochs, dataloaders, dataset_sizes, device):\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = Path(tempdir) / 'best_model_params.pt'\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0 # we will only save the best model\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            logger.info(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            train_loss, train_acc = train(model, dataloaders['train'], lossfn, optimizer, device)\n",
    "            train_loss = train_loss / dataset_sizes['train']\n",
    "            train_acc = train_acc / dataset_sizes['train']\n",
    "            logger.info(f'Train Loss: {train_loss:.4f} Accuracy: {train_acc:.4f}')\n",
    "            test_loss, test_acc = test(model, dataloaders['val'], lossfn, optimizer, scheduler, device)\n",
    "            test_loss = test_loss / dataset_sizes['val']\n",
    "            test_acc = test_acc / dataset_sizes['val']\n",
    "            logger.info(f'Test Loss: {test_loss:.4f} Accuracy: {test_acc:.4f}')\n",
    "            if test_acc > best_acc:\n",
    "                    best_acc = test_acc\n",
    "                    logger.info(f\"New best accuracy: {best_acc:.4f}, saving model\")\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the model predictions\n",
    "=================================\n",
    "\n",
    "Generic function to display predictions for a few images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                title = class_names[preds[j]]\n",
    "                ax.set_title(f'predicted: {title}')\n",
    "                imshow(inputs.cpu().data[j], title)\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning the ConvNet\n",
    "======================\n",
    "\n",
    "Load a pretrained model and reset final fully connected layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "num_ftrs = model.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model = model.to(device)\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "\n",
    "settings = {'step_size' : 7, 'gamma' : 0.1}\n",
    "scheduler = lr_scheduler.StepLR(optimizer, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "model_conv = train_model(\n",
    "    model = model,\n",
    "    lossfn = lossfn,\n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler,\n",
    "    num_epochs = epochs,\n",
    "    dataloaders = dataloaders,\n",
    "    dataset_sizes = dataset_sizes,\n",
    "    device = device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "==================\n",
    "\n",
    "For good results, you should set the amount of epochs to 25. \n",
    "\n",
    "This should take around 15-25 min on CPU. On GPU though (for example, on colab), it takes less\n",
    "than a minute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet as fixed feature extractor\n",
    "==================================\n",
    "\n",
    "Here, we need to freeze all the network except the final layer. We need\n",
    "to set `requires_grad = False` to freeze the parameters so that the\n",
    "gradients are not computed in `backward()`.\n",
    "\n",
    "This has some advantages over finetuning the complete model:\n",
    "- **Faster**: Since we are not computing most of the gradients, the\n",
    "    forward pass is faster.\n",
    "- **Memory Efficient**: The memory required for computing the gradients is\n",
    "    less.\n",
    "- **Less risk of overfitting**: Since most of the model is frozen, the\n",
    "    risk of overfitting on this small dataset is less.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "==================\n",
    "\n",
    "On CPU this will take about half the time compared to previous scenario.\n",
    "This is expected as gradients don\\'t need to be computed for most of the\n",
    "network. However, forward does need to be computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "model_conv = train_model(\n",
    "    model = model_conv,\n",
    "    lossfn = lossfn,\n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler,\n",
    "    num_epochs = epochs,\n",
    "    dataloaders = dataloaders,\n",
    "    dataset_sizes = dataset_sizes,\n",
    "    device = device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_conv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
